---
Title: ' LDAの各変数の意味と幾何的解釈について'
Category:
- トピックモデル
- 自然言語処理
Date: 2015-05-04T16:39:58+09:00
URL: http://ni66ling.hatenadiary.jp/entry/2015/05/04/163958
EditURL: https://blog.hatena.ne.jp/ni66ling/ni66ling.hatenadiary.jp/atom/entry/8454420450093450226
---

## はじめに
LDAの仕組みについて，時間をあけるとすぐに記憶が飛んでしまうためメモ．  
ここでは以下についてまとめます((ほとんどの内容を本ページ末尾の参考文献から引っ張ってきています))

* LDAのグラフィカルモデルにおける各変数の意味とは？
* LDAは幾何的に何をやってるのか？

## LDAのグラフィカルモデル

まず，各文書についてBag of Words(BoW)表現に変換する((文書を形態素解析し，形態素のヒストグラムに変換する．例えば，文書が「もももすももももものうち」であれば，分かち書き「もも/も/すもも/も/もも/の/うち」に対して，形態素ごとにカウントした「もも:2 も:2 すもも:1 の:1 うち:1」がBoW表現である．))．  
そして，次の仮定をおく．

* 文書は複数のトピック((話題．音楽，スポーツ，政治…など))から構成され，その構成比を離散分布としてもつ
* トピックは語彙の出現確率分布で表現される
* 単語ごとに潜在トピックが存在する((出現箇所が異なればその単語のトピックは異なる．例えば，「アップルが新製品を発表しました」と「バナナ，アップル，キウイのミックスジュースがうまい」の「アップル」のトピックは異なる．前者は「企業」トピックとしてのアップルであり，後者は「フルーツ」トピックとしてのアップルである．))

これをグラフィカルモデルに落としこむと下図になる((これは正確にはSmoothed LDAのグラフィカルモデル.無印LDAの場合はβが存在しない．多くの場合，LDAといえばSmoothed LDAを指すみたい．))．

![Smoothed LDAのグラフィカルモデル](http://f.st-hatena.com/images/fotolife/n/ni66ling/20150504/20150504032913_original.png?1430677772)

すると，グラフィカルモデルにおける各変数の意味は次のようになる．((ベクトル表記に関して，上図では太字で記し，下では矢印で表記する．…はてなtex記法で太字を表現できなかった…))

* [tex:M]：文書数
* [tex:K]：文書集合全体におけるトピック数
* [tex:V]：文書集合全体における語彙数((ここでは「語彙」はユニークな形態素として呼び，一方「単語」はユニークでない形態素として呼ぶ．例えば，「もももすももももものうち」の語彙数は「もも」「も」「すもも」「の」「うち」の5つとするのに対して，単語数は「もも」「も」「すもも」「も」「もも」「の」「うち」の7つとする．))
* [tex:n_d]：文書dにおける単語数((語彙数とは異なるので注意))
* [tex:w_{d,i}\in\left\\{1,..,V\right\\}]：文書[tex:d]における[tex:i]番目の単語の語彙インデックス((語彙インデックスは語彙へのマッピング情報を持つ．例えば，語彙インデックス「3」は語彙「すもも」を表すなど))
* [tex:z_{d,i} \in \left\\{1,..,K\right\\}]：文書[tex:d]における[tex:i]番目の単語の潜在トピック
* [tex:\vec{\theta_d} = \left( \theta\_{d,1},..,\theta\_{d,K} \right)]：文書[tex:d]におけるトピックの出現確率ベクトル．[tex:\sum_{k=1}^{K}\theta\_{d,k}=1]
* [tex:\vec{\alpha} = \left( \alpha\_1,..,\alpha\_K \right)]：トピックの出現頻度の偏りを表すパラメータ((要素値[tex:\alpha_k]は大きくなるに連れて，文書集合全体においてトピック[tex:k]が出現しやすくなる))
* [tex:\vec{\phi_k} = \left( \phi\_{k,1},..,\phi\_{k,V} \right)]：トピック[tex:k]における語彙の出現確率ベクトル．[tex:\sum_{v=1}^{V}\phi\_{k,v}=1]
* [tex:\vec{\beta} = \left( \beta\_1,..,\beta\_V \right)]：語彙の出現頻度の偏りを表すパラメータ((要素値[tex:\beta_v]は大きくなるに連れて，文書集合全体において語彙インデックス[tex:v]の語彙が出現しやすくなる))

## LDAの生成過程

* [tex:\vec{\theta_d} \sim {\rm Dir}(\vec{\alpha})\ \ \ \ \ \ \ (d=1,..,M)]
* [tex:\vec{\phi_k} \sim {\rm Dir}(\vec{\beta})\ \ \ \ \ \ \ (k=1,..,K)]
* [tex:z_{d,i} \sim {\rm Multi}(\vec{\theta\_{d}})\ \ \ \ \ \ \ (i=1,..,n\_d)]
* [tex:w_{d,i} \sim {\rm Multi}(\vec{\theta\_{z\_{d,i}}})\ \ \ \ \ \ \ (i=1,..,n\_d)]

ただし，[tex:{\rm Dir}]はディリクレ分布，[tex:{\rm Multi}]は多項分布を表す．((各分布の意味はこちらのスライドがわかりやすいです → ３分でわかる多項分布とディリクレ分布 - http://www.slideshare.net/stjunya/ss-29629644/4))  
また，[tex:\vec{\alpha}]と[tex:\vec{\beta}]はパラメータとして与える．

##### 補足：パラメータ[tex:\vec{\alpha}]と[tex:\vec{\beta}]の意味

多項分布による生成過程[tex:x_i \sim {\rm Multi}(x\_i\|\vec{\pi}),(i=1,..,n)]を考えて，  
[tex:\vec{\pi}]の事前分布としてディリクレ分布[tex:p(\vec{\pi}|\vec{\alpha})={\rm Dir}(\vec{\pi}|\vec{\alpha})]，ただし[tex:\vec{\alpha}=(\alpha\_1,..,\alpha\_K)]を考える．  

すると，[tex:\vec{\pi}]の事後分布は，[tex:p(\vec{\pi}|\vec{x},\vec{\alpha})={\rm Dir}(\vec{\pi}|\vec{\alpha}+\vec{n})]となる．  
ただし，[tex:\vec{n}=(n\_1,..,n\_K),n_k]は[tex:n]回試行の中で[tex:k]が出現した回数．  
すなわち，事前分布における[tex:\vec{\alpha}]は，事後分布では[tex:\vec{n}]に加算される．  
したがって，[tex:\vec{\alpha}]はデータを観測する前の[tex:k]ごとの仮想的頻度を表す．  

よって，LDAにおける[tex:\vec{\alpha}]と[tex:\vec{\beta}]についていえば，次のようになる．
  
* [tex:\vec{\alpha}]は，トピックの出現頻度の偏りを表すパラメータ  
* [tex:\vec{\beta}]は，語彙の出現頻度の偏りを表すパラメータ

## LDAの幾何的解釈
![LDAの幾何的解釈](http://f.st-hatena.com/images/fotolife/n/ni66ling/20150504/20150504153208_original.png?1430721137)
簡単のため，全文書集合における語彙数Vが3の場合を考える((すべての文書において，登場する語彙がgame,play,musicの3つしか存在しない状況))．  
このとき，各文書をBoW表現にすることで，上図の語彙座標単体((各語彙の出現確率の総和が1となるような空間))に射影できる．  

各文書は語彙座標単体空間に偏りをもって分布していると想定((単語には共起性があるから))され，より低次元に射影できるはずである．  
この低次元空間（単体）を潜在トピック座標単体と呼び，その基底ベクトルがトピックとなり，トピックは語彙の出現確率分布で表現される．そして，各文書のトピック分布は，潜在トピック座標単体上の点となる．  

つまり，LDAのは，文書集合の潜在トピック座標単体上への射影とみることができ，  
潜在トピック座標単体は，単語座標単体よりも低次元であるため次元圧縮と見ることができる．

## 参考文献
[asin:4339027588:detail]
